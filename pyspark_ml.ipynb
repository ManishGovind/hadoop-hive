{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgFE1X+g22GTMl2e6P0yZD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManishGovind/hadoop-hive/blob/master/pyspark_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: install pyspark\n",
        "\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcNIP6DQOoRm",
        "outputId": "8500f72c-be4a-4c42-f717-9f397f94f58c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=bf37bed48b5acce56d35c4aea767e08c8af2b9460dd78aca28244979662a20be\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tIVej-8AOjcs",
        "outputId": "39fcdcbe-bf19-4cc4-bc18-de9c9f9ff249"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/ratings.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-bbb52950b5fa>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GenreBasedMovieRating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/movies.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/ratings.csv."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.appName(\"GenreBasedMovieRating\").getOrCreate()\n",
        "ratings = spark.read.csv('/content/ratings.csv', inferSchema=True,header=True)\n",
        "movies = spark.read.csv('/content/movies.csv', inferSchema=True,header=True)\n",
        "\n",
        "\n",
        "\n",
        "# Join ratings and movies on movie_id to get genre information\n",
        "data = ratings.join(movies, \"movieId\")\n",
        "data.show()\n",
        "indexer = StringIndexer(inputCol=\"genre\", outputCol=\"genre_index\")\n",
        "data_indexed = indexer.fit(data).transform(data)\n",
        "#data_indexed.show()\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"genre_index\"],\n",
        "outputCol=\"features\")\n",
        "data_assembled = assembler.transform(data_indexed)\n",
        "\n",
        "data_assembled.show()\n",
        "train_data, test_data = data_assembled.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train a decision tree regressor\n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"rating\")\n",
        "\n",
        "# Train the model\n",
        "dt_model = dt.fit(train_data)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = dt_model.transform(test_data)\n",
        "\n",
        "# Evaluate the model using RMSE\n",
        "evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL ANAlysis"
      ],
      "metadata": {
        "id": "UPlifr-e8vvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, desc , avg\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "from pyspark.ml.recommendation import ALS"
      ],
      "metadata": {
        "id": "C9WyQXDL8vWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SQL Analysis').getOrCreate()\n",
        "\n",
        "#movies_csv = '/content/movies.csv'\n",
        "# ratings_csv = '/content/ratings.csv'\n",
        "\n",
        "# # Define schema\n",
        "# schema = \" date STRING, delay INT, distance INT, \\\n",
        "# origin STRING, destination STRING\"\n",
        "# # Read and create a temporary view\n",
        "# df = (spark.read.format(\"csv\") \\\n",
        "# .option(\"header\", \"true\") \\\n",
        "# .schema(schema) \\\n",
        "# .load(csv_file))\n",
        "# df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
        "\n",
        "\n",
        "# Load the data\n",
        "\n",
        "\n",
        "movies = spark.read.csv('/content/movies.csv', inferSchema=True, header=True)\n",
        "ratings = spark.read.csv('/content/ratings.csv', inferSchema=True, header=True)\n",
        "\n",
        "# Create a new database named movie_ratings and load the two files created in Step 1 into\n",
        "# managed tables movies and ratings.\n",
        "movies.createOrReplaceTempView(\"movies\")\n",
        "ratings.createOrReplaceTempView(\"ratings\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Calculate the total number of movies and ratings.\n",
        "total_movies = spark.sql(\"SELECT COUNT(*) AS total_movies FROM movies\")\n",
        "total_ratings = spark.sql(\"SELECT COUNT(*) AS total_ratings FROM ratings\")\n",
        "total_movies.show()\n",
        "total_ratings.show()\n",
        "\n",
        "# Find the average rating for each movie.\n",
        "average_ratings = spark.sql(\"SELECT movieId, AVG(rating) AS average_rating FROM ratings \\\n",
        "    GROUP BY movieId \\\n",
        "\")\n",
        "\n",
        "# Determine the top-rated movies.\n",
        "top_ratings = spark.sql(\"SELECT movieId, AVG(rating) AS top_rating FROM ratings \\\n",
        "    GROUP BY movieId \\\n",
        "      ORDER BY top_rating DESC \\\n",
        "\")\n",
        "average_ratings.show(10)\n",
        "top_ratings.show(10)\n",
        "\n",
        "# Using window functions, compute the overall movies rating rank per genre.\n",
        "window_spec = Window.partitionBy(\"genre\").orderBy(col(\"average_rating\").desc())\n",
        "combined_table = spark.sql(\" SELECT m.movieId, m.genre, AVG(r.rating) AS average_rating \\\n",
        "        FROM movies m \\\n",
        "        JOIN ratings r ON m.movieId = r.movieId \\\n",
        "        GROUP BY m.movieId,  m.genre \\\n",
        "   \" )\n",
        "movies_rank_per_genre = combined_table.withColumn(\"rank\", rank().over(window_spec))\n",
        "movies_rank_per_genre.show()\n",
        "\n",
        "# Identify users who have rated movies from a wide range of genres.\n",
        "wide_range_users = spark.sql(\"\"\"\n",
        "    SELECT userId, COUNT(DISTINCT genre) AS num_genres_rated\n",
        "    FROM ratings r JOIN movies m ON r.movieId = m.movieId\n",
        "    GROUP BY userId\n",
        "    ORDER BY num_genres_rated DESC\n",
        "\"\"\")\n",
        "wide_range_users.show()\n",
        "\n",
        "\n",
        "average_ratings_per_genre = spark.sql(\"\"\"\n",
        "    SELECT genre, AVG(r.rating) AS average_rating\n",
        "    FROM ratings r\n",
        "    JOIN movies m ON r.movieId = m.movieId\n",
        "    GROUP BY genre\n",
        "    ORDER BY average_rating DESC\n",
        "\"\"\")\n",
        "average_ratings_per_genre.show()\n",
        "\n",
        "# Create an ALS model\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
        "model = als.fit(ratings)\n",
        "\n",
        "# Generate top 10 movie recommendations for a given user\n",
        "user_id = 1\n",
        "recommendations = model.recommendForUserSubset(ratings.filter(ratings.userId == user_id), 10) # N can be changed to get the top N movies\n",
        "\n",
        "# Print the recommendations\n",
        "recommendations.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "un0LFlQO8vSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Calculate the average rating for each genre and identify the genres with the highest average\n",
        "# ratings.\n",
        "average_ratings_per_genre = spark.sql(\" SELECT genre, AVG(r.rating) AS average_rating \\\n",
        "    FROM ratings r \\\n",
        "    JOIN movies m ON r.movieId = m.movieId \\\n",
        "    GROUP BY genre \\\n",
        "    ORDER BY average_rating DESC \\\n",
        "\")\n",
        "average_ratings_per_genre.show(10)\n",
        "\n",
        "#Find movies with a high average rating but a low number of ratings.\n",
        "\n",
        "high_average_low_count_movies = spark.sql(\" SELECT m.title, m.genre, AVG(r.rating) AS average_rating, COUNT(*) AS rating_count \\\n",
        "    FROM movies m \\\n",
        "    JOIN ratings r ON m.movieId = r.movieId \\\n",
        "    GROUP BY m.title, m.genre \\\n",
        "    HAVING AVG(r.rating) >= (SELECT AVG(rating) FROM ratings) \\\n",
        "           AND COUNT(*) <= 4 \\\n",
        "\")\n",
        "high_average_low_count_movies.show()\n",
        "\n",
        " #Find genres that have relatively low average ratings compared to other genres.\n",
        "\n",
        "low_average_ratings_genres = spark.sql(\" SELECT genre, AVG(r.rating) AS average_rating \\\n",
        "    FROM movies m \\\n",
        "    JOIN ratings r ON m.movieId = r.movieId \\\n",
        "    GROUP BY genre  \\\n",
        "    HAVING AVG(r.rating) < (SELECT AVG(rating) FROM ratings) \\\n",
        "\")\n",
        "low_average_ratings_genres.show()"
      ],
      "metadata": {
        "id": "AUyMTizLAts2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: highest rated movie for each genre\n",
        "\n",
        "highest_rated_movies_per_genre = spark.sql(\"\"\"\n",
        "    SELECT m.genre, m.title, MAX(r.rating) AS highest_rating\n",
        "    FROM movies m\n",
        "    JOIN ratings r ON m.movieId = r.movieId\n",
        "    GROUP BY m.genre, m.title\n",
        "    ORDER BY m.genre, highest_rating DESC\n",
        "\"\"\")\n",
        "\n",
        "highest_rated_movies_per_genre.show()\n"
      ],
      "metadata": {
        "id": "R_oD__WHQ8P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree Regressor\n"
      ],
      "metadata": {
        "id": "IdLy_ah06YVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DecisionTrees\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "Jj1lQsxGQ4ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data\n",
        "filePath = \"/content/sf-airbnb-clean.parquet/\"\n",
        "airbnbDF = spark.read.parquet(filePath)\n",
        "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\", \"number_of_reviews\", \"price\").show(5)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "\n",
        "\n",
        "trainDF, testDF = airbnbDF.randomSplit([0.8, 0.2], seed=42)\n",
        "print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbDSBDsP8bre",
        "outputId": "d4958304-1e02-4c7a-f386-3b40ec572332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---------------+--------+---------+-----------------+-----+\n",
            "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
            "+----------------------+---------------+--------+---------+-----------------+-----+\n",
            "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
            "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
            "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
            "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
            "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
            "+----------------------+---------------+--------+---------+-----------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "There are 5780 rows in the training set, and 1366 in the test set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DecisionTreeRegressor\n",
        "#dt = DecisionTreeRegressor(labelCol=\"price\")\n",
        "dt = RandomForestRegressor(labelCol=\"price\" , maxDepth=10, numTrees=20)\n",
        "\n",
        "# Index categorical columns\n",
        "categoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\n",
        "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
        "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
        "\n",
        "# Assemble features\n",
        "numericCols = [field for (field, dataType) in trainDF.dtypes if (dataType == \"double\") and (field != \"price\")]\n",
        "assemblerInputs = indexOutputCols + numericCols\n",
        "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "\n",
        "# Define pipeline stages\n",
        "stages = [stringIndexer, vecAssembler, dt]\n",
        "pipeline = Pipeline(stages=stages)\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "dt.setMaxBins(40)  # Set max bins for Decision Tree\n",
        "pipelineModel = pipeline.fit(trainDF)\n",
        "\n",
        "# Retrieve the trained DecisionTreeRegressor model\n",
        "dtModel = pipelineModel.stages[-1]\n",
        "print(dtModel.toDebugString)"
      ],
      "metadata": {
        "id": "gZWpyqkJ-6kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Feature Importances\n",
        "dtModel = pipelineModel.stages[-1]\n",
        "#print(dtModel.toDebugString)\n",
        "\n",
        "# Create DataFrame of Feature Importances\n",
        "schema = \"feature STRING, importance FLOAT\"\n",
        "feature_importances_float = [float(val) for val in dtModel.featureImportances]\n",
        "featureImp = spark.createDataFrame(list(zip(vecAssembler.getInputCols(), feature_importances_float)), schema)\n",
        "featureImp.orderBy(\"importance\", ascending=False).show()\n",
        "\n",
        "# Make Predictions\n",
        "predDF = pipelineModel.transform(testDF)\n",
        "predDF.select(\"price\", \"prediction\").show()\n",
        "\n",
        "# Evaluate Model\n",
        "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
        "rmse = regressionEvaluator.evaluate(predDF)\n",
        "print(f\"RMSE is {rmse:.1f}\")\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOO22dIcAGus",
        "outputId": "648f94e5-3a4e-4e47-a44f-7d010f28e94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|             feature| importance|\n",
            "+--------------------+-----------+\n",
            "|neighbourhood_cle...| 0.13150403|\n",
            "|      minimum_nights|0.099471994|\n",
            "|            bedrooms| 0.09508264|\n",
            "|        accommodates|  0.0902129|\n",
            "|   number_of_reviews| 0.07730515|\n",
            "|cancellation_poli...| 0.06669854|\n",
            "|                beds| 0.06279356|\n",
            "|host_total_listin...|0.058785368|\n",
            "|            latitude|0.045034967|\n",
            "|instant_bookableI...|0.038089752|\n",
            "|           longitude|0.034680534|\n",
            "|  property_typeIndex| 0.03272191|\n",
            "|review_scores_rating| 0.02939379|\n",
            "|           bathrooms| 0.02686555|\n",
            "| review_scores_value|0.023565233|\n",
            "|      room_typeIndex| 0.02064288|\n",
            "|review_scores_loc...|0.010091712|\n",
            "|host_is_superhost...|0.008890638|\n",
            "|review_scores_val...|0.008696568|\n",
            "|review_scores_rat...|0.008357036|\n",
            "+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------+------------------+\n",
            "| price|        prediction|\n",
            "+------+------------------+\n",
            "|  85.0|136.35044810693128|\n",
            "|  45.0| 71.21938842659809|\n",
            "|  70.0| 71.21938842659809|\n",
            "| 128.0| 88.62308235638406|\n",
            "| 159.0|114.98068525521994|\n",
            "| 250.0|205.01412976437615|\n",
            "|  99.0|171.49152973027745|\n",
            "|  95.0| 187.6406873578847|\n",
            "| 100.0| 289.3608320887455|\n",
            "|2010.0| 279.5724553527999|\n",
            "| 270.0|174.44915270728274|\n",
            "| 500.0| 883.3296551009283|\n",
            "| 125.0| 138.9769390173965|\n",
            "| 210.0| 454.9693664145995|\n",
            "|  60.0|125.87146511100575|\n",
            "| 170.0|  371.010366271604|\n",
            "| 214.0|178.72294489918445|\n",
            "| 120.0|173.60043730450542|\n",
            "|  82.0|168.49896344564104|\n",
            "| 169.0| 396.3239083853662|\n",
            "+------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "RMSE is 238.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J82gJ1zxC-c8",
        "outputId": "40f92c36-eee9-40f4-f930-fe6f10492456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark-nlp in /usr/local/lib/python3.10/dist-packages (5.3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K fold Validation"
      ],
      "metadata": {
        "id": "zB6iP1R_CuEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder , CrossValidator\n",
        "\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        ".addGrid(dt.maxDepth, [2, 4, 6])\n",
        ".addGrid(dt.numTrees, [10, 100])\n",
        ".build())\n",
        "\n",
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
        "cv = CrossValidator(estimator=pipeline,\n",
        " evaluator=evaluator,\n",
        " estimatorParamMaps=paramGrid,\n",
        " numFolds=3,seed=42)\n",
        "\n",
        "cvModel = cv.fit(testDF)\n",
        "print(list(zip(cvModel.getEstimatorParamMaps(),cvModel.avgMetrics)))"
      ],
      "metadata": {
        "id": "nNEIRa4PCtqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(zip(cvModel.getEstimatorParamMaps(),cvModel.avgMetrics)))"
      ],
      "metadata": {
        "id": "d4esqV38C6_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}